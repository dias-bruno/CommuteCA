---
title: "Creating synthetic instrumental variables from spatial filtering"
subtitle: "a methodology for Canadian regions based on the 2021 Census of Population" # only for html output
author: "Bruno Santos & Antonio Paez"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r clean-workspace, include=FALSE}
# cleaning objects from the workspace 
rm(list = ls())
```

```{r setup, include=FALSE}
# layout configuration 
library(tufte)
library(knitr)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Introduction

This Rmarkdown file is part of the
[**CommuteCA**](https://github.com/dias-bruno/CommuteCA) package. This
package was created in conjunction with the office of the [*Research
Data Center* at *McMaster University*](https://rdc.mcmaster.ca/), the
[*Sherman Centre for Digital Scholarship*](https://scds.ca/) and the
[*Mobilizing Justice*](https://mobilizingjustice.ca/)[^1].

[^1]: For this demonstration, we will use the case of the city of
    Toronto. If you want to use the other options, the original data
    from the Census in an RDC office or the test data for all locations
    in Canada, *update* the address in the chunk.

The [**CommuteCA**](https://github.com/dias-bruno/CommuteCA) R package
was created to develop standardized methods for transport analysis in
research, particularly for analysis using the [*2021 Census of
Population*](https://www12.statcan.gc.ca/census-recensement/index-eng.cfm)
from Statistics Canada. We focused our efforts on the [*Commuting
Reference
Guide*](https://www12.statcan.gc.ca/census-recensement/2021/ref/98-500/011/98-500-x2021011-eng.cfm),
which provides valuable variables and information on commuting for the
Canadian population aged 15 and older living in private households.

After identifying disadvantaged populations in terms of job
accessibility, we will now advance our analysis by developing regression
models to explore the relationship between job accessibility and
individual outcomes. These models examine the connection between a
dependent variable (an outcome) and predictor variables (job
accessibility indicators and other socioeconomic information). They
estimate the probability of the dependent variable, conditional on a
linear combination of the predictor variables.

In social science, it is well understood that "correlation does not
imply causality" [@cunningham2021]. Therefore, it is important to
evaluate the actual impact of a predictor variable (the cause) on the
outcome (the effect). This requires studying the process that determines
the cause-and-effect relationship between variables, a process known as
*causal inference*.

Including causal inference in our modeling approach means we must
account for the possible (and indeed likely) presence of *endogeneity*.
Endogeneity arises when the effect of an independent variable on a
dependent variable cannot be interpreted causally, often due to omitted
variables that lead to biased (i.e., inconsistent) estimates
[@antonakis2010]. ndefinedThere are various ways to address endogeneity.
In our case, we propose using an instrumental variable.

In this section, we will create synthetic instrumental variables
obtained after applying a spatial filtering to the spatial availability
variables created in the previous sections, in a methodology created by
Gallo and Páez [-@legallo2013].

## Suggested Readings

-   Le Gallo, J., & Páez, A. (2013). Using synthetic variables in
    instrumental variable estimation of spatial series models.
    *Environment and Planning A*, *45*(9), 2227-2242.
-   Cunningham, S. (2021). *Causal inference: The mixtape*. Yale
    university press.

# Instrumental variables

A fundamental assumption in regression analysis is the absence of
correlation between the explanatory variables and the error terms
[@legallo2013]. Endogeneity occurs when an independent variable
correlates with the regression error term. To predict outcomes from the
2021 Census of Population, we propose to address endogeneity by applying
an instrumental variable technique. When an endogenous variable is
included in a regression model, the consistency of the estimators is
compromised, affecting the results and hindering the ability to
establish a cause-effect relationship between the explanatory and
dependent variables.

An instrumental variable Z must satisfy the following criteria
([@cunningham2021; @luz2022]):

1.  Z must be highly correlated with and have a causal effect on the
    endogenous variable D, or share a common cause with D.

2.  Z must affect the dependent variable Y only through D, with no
    direct effect of Z on Y.

3.  Z must not be correlated with the regression residuals of D on Y.

A good instrument for the proposed models must be highly correlated with
and have a causal effect (or share a common cause) on the accessibility
variable (the endogenous variable); affect employment outcomes only
through accessibility; and not share common causes with employment
outcomes. In simpler terms, the instrument must be correlated with
employment outcomes only through accessibility.

Examples of instrumental variables used in studies analyzing the
relationship between employment outcomes and job accessibility include
Euclidean distance from the spatial unit centroid to the nearest major
road or employment subcenter ([@delmelle2021; @jin2018]), share of
no-vehicle households ([@hu2017]), transit accessibility and car
unavailability ([@johnson2017]), distance to rivers ([@duarte2023]), and
population density ([@bastiaanssen2021]).

However, two important challenges arise in the search for relevant
instruments:

1.   The likelihood that the instruments and error terms are correlated
    increases as the correlation between the endogenous variable and the
    instruments becomes stronger, which can render the instruments
    invalid.

2.  Instruments that are only weakly correlated with the endogenous
    variable may be ineffective and perform poorly.

In response to these challenges, Le Gallo and Páez ([-@legallo2013])
developed an alternative, cutting-edge approach that relies on synthetic
instrumental variables when the model involves spatial data. The
researchers demonstrated that synthetic variables can be generated using
spatial filtering, a technique that removes spatial residual
autocorrelation to improve regression analysis. They propose the use of
an eigenvector-based technique, in which transforms a projection of the
contiguity matrix into latent map patterns through eigenvector
structural analysis. These latent patterns, included in the spatial
structure of the system, are then used to generate synthetic variables,
which serve as instruments in IV estimation. Since they are derived from
the spatial structure of the system, these synthetic instruments are
exogenous, thereby fulfilling the core requirement of IV estimation.

We have chosen to use this technique because, given our aim to create a
methodology that can be applied to any Canadian city, there are no
universally applicable candidates for instrumentation that could be
obtained in every context.

# Spatial Filtering

According to Le Gallo and Páez [-@legallo2013], spatial filtering is a
method to deal with, or filter from the residuals, spatial
autocorrelation present during regression analysis. Spatial
autocorrelation is the degree of similarity among neighboring
observation. Usually, the definition of the neighboring (ie, spatial
structure) is made by the use of spatial weight matrix W, with
individual values represented by:

$$
w_{ij} = \begin{cases}    w_{ij} > 0, & \text{if i and j are neighbors},\\    w_{ij} = 0, & \text{otherwise.}\end{cases}
$$

Neighborhood can be defined on the basis of contiguity, distance, or
length of shared edge, among other criteria. So, having a weighted
matrix that represents the spatial structure, the steps to generate a
filter can be defined as follows:

1.  Build a spatial weights matrix to capture the distances between all
    pairs of spatial units (in our case, Dissemination Areas, or DAs)
    under analysis.
2.  Select a distance threshold to identify neighbors for each DA. This
    process usually results in a binary spatial contiguity matrix. There
    is no clear theoretical guidance on the optimal distance threshold
    for constructing the binary contiguity matrix.
3.  Compute all eigenvectors associated with the contiguity matrix.
    These eigenvectors are orthogonal to one another and each represents
    a portion of the variance in the spatial contiguity matrix. Each
    independent variable will have its own set of associated
    eigenvectors.
4.  Construct the spatial filter as a linear combination of a subset of
    these eigenvectors. The spatial filter for each independent variable
    is typically obtained by regressing the independent variable on the
    subset of eigenvectors with p-values below a specified threshold
    (for instance, p ≤ 0.05), and then using the predicted values of the
    independent variable as the synthetic instrument.

The pseudo-code to build the spatial filter is as follows:

-   Initialize an index value $i = 1$, an empty vector for the spatial
    filter $S = 0$, and a constant vector $X = 1$ .
-   In a for-loop and for every $i <= n$, with $n$ being number of
    eigenvectors, select the eigenvector $E_i$ obtained from the
    weighted matrix as a candidate for inclusion in the spatial filter,
    and estimate the model $ Y = \theta X + \\beta E_i + \epsilon $
    using Ordinary Least Squares (OLS), where $\theta$ and $\beta$
    are vectors of coefficients, and $\epsilon$ is a vector of error
    terms.
-   If coefficient $i$ is significant at a pre-determined level (eg,
    $p-value <= 0.05$), then synthesize the eigenvector and the existing
    filter: $S = S + \beta E_i$.
-   The spatial filter $S$ is the synthetic instrumental variable.

In summary, a spatial filter will be created to generate a synthetic
counterpart that accurately reproduces a spatial random variable. To
explore the potential of using eigenvectors to build synthetic
instrumental variables, we will not only construct a spatial filter
based on the contiguity matrix but also create additional filters based
on commuting duration. Specifically, we will use a travel time matrix -
since commute duration also depends on the spatial structure and
represents an explicit space-time relationship. Additionally, we will
develop a spatial filter based on commuting time impedance values that
were derived after applying impedance functions (calculated from spatial
accessibility) to the travel time matrix.

# Let's code!

Load the packages:

```{r load-packages}
library(CommuteCA)
library(dplyr)# A Grammar of Data Manipulation 
library(fitdistrplus) # Help to Fit of a Parametric Distribution to Non-Censored or Censored Data
library(scales) # Scale data column-wise in a computationally efficient way
library(here) # enable easy file referencing in project-oriented workflows
library(ggplot2) # Create Elegant Data Visualizations Using the Grammar of Graphics
library(RColorBrewer) # color schemes for maps (and other graphics) designed
library(sf) # support for simple features, a standardized way to encode spatial vector data
library(tmap) # thematic maps
library(tidyr) # tidying data
library(matlib) # To get the eigenvectors  
library(Hmisc) # To considerate weights when calculate the median
library(sf) # support for simple features, a standardized way to encode spatial vector data
library(spdep) # Construct neighbours list from polygon list
```

## Data

The dataset used in this demonstration is test data produced to
replicate the variables available in the original Census of Population
for the City of Toronto. The test data contains 52,650 rows and 31
columns. As in the original census data, each row refers to a respondent
and each column refers to a variable[^2]. The creation of test data was
necessary because the surveys provided by Statistics Canada are
confidential and cannot be accessed outside of a Research Data Center.

[^2]: You can check out more information about the Census on the
    [Dictionary
    website](https://www12.statcan.gc.ca/census-recensement/2021/ref/dict/index-eng.cfm).

If you want to work with the original Census dataset, the process for
obtaining the accessibility measures will be the same as for the test
data, except that you will have to update the address of the file in the
chunk[^3] called *load-census-data*.

[^3]: A code chunk is an executable part of the R code

For this R markdown, we'll use the following variables[^4]:

[^4]: The explanation of each variable can be found in the [*2021 Census
    of Population's
    website*](https://www12.statcan.gc.ca/census-recensement/index-eng.cfm).

|  |  |
|-------------------------|-----------------------------------------------|
| **Variable** | **Description** |
| PRCDDA | Refers to the dissemination area (DA) of current residence. |
| PCD | Census division of current residence. |
| CompW1 | Weight for the households and dwellings universes. |
| PWDA | Place of work dissemination area. |
| PWCD | Place of work census division. |
| PWDUR | Commuting duration, it refers to the length of time, in minutes, usually required by a person to travel to their place of work. |
| PwMode | Main mode of commuting' refers to the main mode of transportation a person uses to travel to their place of work. |
| PWDist | The straight-line distance, in kilometres, between a person's residence and his or her usual place of work. |

: Census variables used in this R markdown.

A dissemination area (DA) is a small, relatively stable geographic unit
composed of one or more adjacent dissemination blocks. It is the
smallest standard geographic area for which all census data are
disseminated. DAs cover all Canadian territory.

We will also use the travel time table with the impedance values,
created and exported during the modelling of job accessibility. We are
interested in the following variables of the travel time matrix:

| **Variable** | **Description** |
|-------------------|-----------------------------------------------------|
| from_id | Refers to the DA defined as the origin. |
| to_id | Refers to the DA defined as the destination. |
| travel_time | Estimated travel time in minutes from origin to destination. |
| PwMode | Transportation mode used to calculate the travel time. |
| f | The impedance value obtained for the travel time in `travel_time` column. |

: Travel time table variables used in this R markdown.

Also, we will use the job spatial availability measures, also obtained
after generating the accessibility models. The job spatial availability
file has the following variables:

| **Variable** | **Description**                                        |
|--------------|--------------------------------------------------------|
| PRCDDA       | Refers to the DA defined as the origin.                |
| PwMode       | Transportation mode used to calculate the travel time. |
| SA_im        | Job spatial availability for the DA.                   |


## Reading files

### Census

Reading census data and creating a R data frame[^5]:

[^5]: For this demonstration, we will use the case of the city of
    Toronto. If you want to use the other options, the original data
    from the Census in an RDC office or the test data for all locations
    in Canada, *update* the address in the chunk.

```{r}
# For the original Census data set
files_address <- paste0(here(),"/data-ignore-inputs/census_test_toronto.csv") # insert-the-RDC-address!!!
census <- read.csv(files_address, header = TRUE)
```

If the original Census dataset available in the RDC is not in .csv
(comma-separated values) format but is instead provided in other formats
such as SPSS, SAS, or SAS Data, you can use the foreign package (a
built-in R library) to import it:

```{r}
# library(foreign)
# foreign::read.dta(files_address) # For Stata
# foreign::read.spss(files_address) # For SPSS
# There are many other options! You can search for this library in the 'Packages' window and explore additional functions for reading your file. 
```

Or if you have installed *CommuteCA*, you can access the *test* data:

```{r load-census-data}
# data("census_test_toronto")
# census <- census_test_toronto
```

| ⚠️**NOTE:** If the code above did not run correctly, you probably are experiencing a file address error. Try to identify the correct address and update the chunk named `census-file-address` to continue.

It's possible to filter the data frame by administrative (province
and/or census division) and/or statistical (census metropolitan areas
and census agglomerations)[^6]. The chunk below shows how to make this
procedure:

[^6]: As said before, we will perform our analysis for the city of
    Toronto. If you want to select a specific area to work, uncomment
    the code above that applies to your case and select the apropriate
    code of your interest unit. Please, check the dictionary to have
    more informations about the [provinces
    code](https://www12.statcan.gc.ca/census-recensement/2021/ref/dict/tab/index-eng.cfm?ID=t1_8),
    [census
    divisions](https://www12.statcan.gc.ca/census-recensement/2021/ref/dict/az/Definition-eng.cfm?ID=geo008),
    and [census metropolitan
    areas](https://www12.statcan.gc.ca/census-recensement/2021/ref/dict/az/Definition-eng.cfm?ID=geo009)

```{r filter-boundaries}
code <- 3520 # census division of Toronto

# Filtering by census division
 census_filtered <- census %>%
                    filter(PCD == code) # Only select respondents who live in  Toronto
 
census <- census %>%
                    filter(PCD == code) # Only select respondents who live in  Toronto
```

```{r select-variables-census}
census_filtered <- census_filtered %>% 
          dplyr::select("PRCDDA",
                        "PCD",
                        "CompW1",
                        "PwMode",
                        "PWDist",
                        "PWDUR",
                        "PWDA",
                        "PWCD")
```

According to the census code book, the variable 'PwMode' has the
following possible values:

-   -3: Not applicable.
-   1: Car, truck or van - as a driver.
-   2: Car, truck or van - as a passenger.
-   3: Bus.
-   4: Subway or elevated rail.
-   5: Light rail, streetcar or commuter train.
-   6: Passenger ferry.
-   7: Walked.
-   8: Bicycle.
-   9: Motorcycle, scooter or moped.
-   10: Other method.

We'll rename the travel modes to facilitate the readability of the data.
Additionally, we'll remove from our analysis travel modes signed as
'Other methods':

```{r rename-PwMode}
census_filtered <- census_filtered  %>% 
                   filter(PwMode < 10) %>% 
                   mutate(PwMode = case_when(PwMode > 0 & PwMode <= 2 ~ "Car/motor",
                                             PwMode == 9 ~ "Car/motor",
                            PwMode >= 3 & PwMode <= 6  ~ "Transit",
                            PwMode == 7  ~ "Walk",
                            PwMode == 8  ~ "Bike"),
         
         PwMode = factor(PwMode, levels = c("Bike", "Walk", "Car/motor", "Transit"))) %>%
  filter(PwMode %in% c("Bike", "Walk", "Car/motor", "Transit"))

census <- census  %>% 
                   mutate(PwMode = case_when(PwMode > 0 & PwMode <= 2 ~ "Car/motor",
                                             PwMode == 9 ~ "Car/motor",
                            PwMode >= 3 & PwMode <= 6  ~ "Transit",
                            PwMode == 7  ~ "Walk",
                            PwMode == 8  ~ "Bike"),
         
         PwMode = factor(PwMode, levels = c("Bike", "Walk", "Car/motor", "Transit")))
```

```{r}
census_workers <- census_filtered %>% 
  filter(PCD == code  & PWCD == code)
```

```{r}
census_expanded <- expand.grid(PRCDDA = unique(census_filtered$PRCDDA),
                               PwMode = unique(census_filtered$PwMode))
```

### Accessibility measures

Setting the folder with the accessibility measures:

```{r read-accessibility}
# Folder with the accessibility files 
measures_folder <- paste0(here::here(),"/data-raw/output/PCD3520/DA/accessibility-measures/")

# Read file job spatial availability by mode
SA <- read.csv(paste0(measures_folder, "SA_mode_original_RDC.csv"))
```

Preparing the accessibility file:

```{r}
SA_mode <- census_expanded %>% 
  left_join(SA, by = c("PRCDDA","PwMode")) %>%
  mutate(SA_im = replace_na(SA_im, 0))
```

### Travel time matrix

Reading the travel time table with the impedance measures:

```{r read-ttm_f}
# Folder  
ttm_folder <- paste0(here(),"/data-raw/output/PCD3520/DA/travel_times/") # Update it if necessary 

# Read file
ttm_f <- read.csv(paste0(ttm_folder, "ttm_f.csv"))
```

### Spatial files

Reading the dissemination areas spatial file:

```{r read-ttm_f}
# Folder  
directory_spatial_files <- paste0(here(),"/data-raw/output/PCD3520/spatial-files/") # Update it if necessary 

# Read file
da_file <- st_read(paste0(directory_spatial_files, "dissemination_areas.shp"))
```

## Creating folder to save files

Creating a directory to export the figures and tables:

```{r directory-export}
diretorio_export_figures <- paste0(here(),"/data-raw/output/PCD3520/DA/spatial-filter-figures/") # Update the address to export the data, if necessary

if(!dir.exists(diretorio_export_figures)){
  dir.create(diretorio_export_figures, recursive = TRUE)}

diretorio_export_tables <- paste0(here(),"/data-raw/output/PCD3520/DA/spatial-filter-measures/") # Update the address to export the data, if necessary

if(!dir.exists(diretorio_export_tables)){
  dir.create(diretorio_export_tables, recursive = TRUE)}
```

# Spatial filters

## Spatial filter based on the commute distance (straight-line)

```{r}
mtx_distance_binary <- spdep::nb2mat(spdep::poly2nb(da_file), style = "B", zero.policy = TRUE)
```

```{r}
diag(mtx_distance_binary) <- 1

rownames(mtx_distance_binary) <- da_file$DAUID
colnames(mtx_distance_binary) <- da_file$DAUID
```

```{r}
# Calculo and values eigenvectors
ev_dist <- eigen(mtx_distance_binary)

# Pego so os vectors e crio um dataframe 
eigv_df <- as.data.frame(Re(ev_dist$vectors))

# altero o nome das colunas 
colnames(eigv_df) <- paste0("EV", seq_len(ncol(eigv_df)))

# adiciono nesse df de vectors o id do PRCDDA 
weight_matrix_eigv_dist <- cbind(eigv_df, PRCDDA = as.numeric(rownames(mtx_distance_binary)))
```

```{r sf-all-modes}
modes <- unique(census_workers$PwMode)

mode_results_dist <- list()

for (mode in modes) {
  cat("Creating the spatial filter (distance) for the transportation mode:", mode, "\n")

  # Creating a df with the accessibility values
  y <- SA_mode %>%
    filter(PwMode == mode) %>%
    left_join(weight_matrix_eigv_dist, by = c("PRCDDA" = "PRCDDA")) %>%
    mutate(x = 1, Sf_dist = 0)
  
  # Creating the spatial filter (Sf_dist)
  for (ev_name in names(y)[startsWith(names(y), "EV")]) {
    formula <- as.formula(paste("SA_im ~ x + Sf_dist +", ev_name))
    model <- lm(formula, data = y)
    coefs <- summary(model)$coefficients

    if (ev_name %in% rownames(coefs)) {
      p_value <- coefs[ev_name, "Pr(>|t|)"]
      B_value <- coefs[ev_name, "Estimate"]

      if (!is.na(p_value) && p_value < 0.05) {
        y$Sf_dist <- y$Sf_dist + B_value * y[[ev_name]]
      }
    }
  }
 
 mode_results_dist[[mode]] <- y
}
```

Visualizing creating a data frame with all spatial filters and
accessibility:

```{r sf_dist-df}
spatial_filter_dist <- as.data.frame(bind_rows(mode_results_dist, .id = "PwMode")) %>%
  dplyr::select(PRCDDA, PwMode, SA_im, Sf_dist)
```

Pearson correlation between job spatial availability and spatial filter
by transportation mode:

```{r sf-correlations}
correlations <- spatial_filter_dist %>%
  group_by(PwMode) %>%
  summarise(correlation = cor(SA_im, Sf_dist, use = "complete.obs")) 

correlations
```

Creating a scatter plot figure:

```{r sf-dist-plot}
r2_labels <- spatial_filter_dist %>%
  group_by(PwMode) %>%
  summarise(r2 = summary(lm(Sf_dist ~ SA_im))$r.squared) %>%
  mutate(label = paste0("R² = ", round(r2, 3)))

spatial_filter_dist_fig <- ggplot(spatial_filter_dist, aes(x = SA_im, y = Sf_dist)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "#4b84db", linewidth = 0.55) +
  geom_text(data = r2_labels, 
            aes(x = -Inf, y = Inf, label = label), 
            hjust = -0.1, vjust = 1.1, inherit.aes = FALSE) +
  facet_wrap(~ PwMode, scales = "free") +
  theme_minimal() +
  labs(x = "Job spatial availability", y = "Spatial filter based on spatial contiguity") + 
theme_minimal()

# Saving figure
ggsave(file = paste0(diretorio_export_figures,"/spatial_filter_dist_scatterplot.jpg"), 
       plot = spatial_filter_dist_fig,
       width = 16, 
       height = 9, 
       units = "cm", 
       dpi = 300)
```

```{r sf-dist-figure}
knitr::include_graphics(paste0(diretorio_export_figures,"/spatial_filter_dist_scatterplot.jpg"))
```

## Spatial filter based on the commute duration

### Median commmute time

Medians:

```{r distance-medians}
dur_medians <- census_workers %>%
  filter(PwMode %in% c("Bike", "Walk", "Car/motor", "Transit")) %>% 
  group_by(PwMode) %>% 
  dplyr::summarise("Median" =  Hmisc::wtd.quantile(PWDUR, weights = CompW1, probs = 0.5),
            .groups = "drop")

dur_medians
```

After visualizing the creation of a spatial filter for the bike mode, we
will generate the spatial filter for each transportation mode:

```{r sf-all-modes}
mode_results <- list()

for (mode in modes) {
  cat("Creating the spatial filter (median duration based) for the transportation mode:", mode, "\n")
  
  # Mode weight matrix
  weight_matrix <- ttm_f %>%
    filter(PwMode == mode) %>% 
    mutate(threshold = 
             case_when(travel_time <= as.numeric(dur_medians$Median[dur_medians$PwMode == mode]) ~ 1,
                       TRUE ~ 0)) %>% 
    dplyr::select(from_id, to_id, threshold) %>% 
    pivot_wider(names_from = to_id, values_from = threshold, values_fill = 0) %>%
    as.data.frame()
  
  # Compute eigenvectors
  mat <- as.matrix(weight_matrix[, -1])
  ev <- eigen(mat)
  eigv_df <- as.data.frame(Re(ev$vectors))
  colnames(eigv_df) <- paste0("EV", seq_len(ncol(eigv_df)))
  
  weight_matrix_eigv <- cbind(from_id = weight_matrix$from_id, eigv_df)

  # Creating a df with the accessibility values
  y <- SA_mode %>%
    filter(PwMode == mode) %>%
    left_join(weight_matrix_eigv, by = c("PRCDDA" = "from_id")) %>%
    mutate(x = 1, Sf = 0)
  
  # Creating the spatial filter (Sf)
  for (ev_name in names(y)[startsWith(names(y), "EV")]) {
    formula <- as.formula(paste("SA_im ~ x + Sf +", ev_name))
    model <- lm(formula, data = y)
    coefs <- summary(model)$coefficients

    if (ev_name %in% rownames(coefs)) {
      p_value <- coefs[ev_name, "Pr(>|t|)"]
      B_value <- coefs[ev_name, "Estimate"]

      if (!is.na(p_value) && p_value < 0.05) {
        y$Sf <- y$Sf + B_value * y[[ev_name]]
      }
    }
  }
 
 mode_results[[mode]] <- y
}
```

Visualizing creating a data frame with all spatial filters and job
accessibility:

```{r sf-df}
spatial_filter_dur_medians <- as.data.frame(bind_rows(mode_results, .id = "PwMode")) %>%
  dplyr::select(PRCDDA, PwMode, SA_im, Sf) %>% 
  rename(Sf_median_dur = Sf)
```

Pearson correlation between job spatial availability and spatial filter
by transportation mode:

```{r sf-correlations}
correlations <- spatial_filter_dur_medians %>%
  group_by(PwMode) %>%
  summarise(correlation = cor(SA_im, Sf_median_dur, use = "complete.obs")) 

correlations
```

Creating a scatter plot figure:

```{r sf-plot}
r2_labels <- spatial_filter_dur_medians %>%
  group_by(PwMode) %>%
  summarise(r2 = summary(lm(Sf_median_dur ~ SA_im))$r.squared) %>%
  mutate(label = paste0("R² = ", round(r2, 3)))

spatial_filter_dur_medians_fig <- ggplot(spatial_filter_dur_medians, aes(x = SA_im, y = Sf_median_dur)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "#dbb52c", linewidth = 0.55) +
  geom_text(data = r2_labels, 
            aes(x = -Inf, y = Inf, label = label), 
            hjust = -0.1, vjust = 1.1, inherit.aes = FALSE) +
  facet_wrap(~ PwMode, scales = "free") +
  theme_minimal() +
  labs(x = "Job spatial availability", y = "Spatial filter based on median commute duration") + 
theme_minimal()

# Saving figure
ggsave(file = paste0(diretorio_export_figures,"/spatial_filter_dur_median_scatterplot.jpg"), 
       plot = spatial_filter_dur_medians_fig,
       width = 16, 
       height = 9, 
       units = "cm", 
       dpi = 300)
```

```{r sf-figure}
knitr::include_graphics(paste0(diretorio_export_figures,"/spatial_filter_dur_median_scatterplot.jpg"))
```

### Duration impedance values

After visualizing the creation of a spatial filter for the bike mode, we
will generate the spatial filter for each transportation mode:

```{r sf-all-modes}
mode_results <- list()

for (mode in modes) {
  cat("Creating the spatial filter (duration impedance based) for the transportation mode:", mode, "\n")
  
  # Mode weight matrix
  weight_matrix <- ttm_f %>%
    filter(PwMode == mode) %>% 
    dplyr::select(from_id, to_id, f) %>% 
    pivot_wider(names_from = to_id, values_from = f, values_fill = 0) %>%
    as.data.frame()
  
  # Compute eigenvectors
  mat <- as.matrix(weight_matrix[, -1])
  ev <- eigen(mat)
  eigv_df <- as.data.frame(Re(ev$vectors))
  colnames(eigv_df) <- paste0("EV", seq_len(ncol(eigv_df)))
  
  weight_matrix_eigv <- cbind(from_id = weight_matrix$from_id, eigv_df)

  # Creating a df with the accessibility values
  y <- SA_mode %>%
    filter(PwMode == mode) %>%
    left_join(weight_matrix_eigv, by = c("PRCDDA" = "from_id")) %>%
    mutate(x = 1, Sf_imp_dur = 0)
  
  # Creating the spatial filter (Sf)
  for (ev_name in names(y)[startsWith(names(y), "EV")]) {
    formula <- as.formula(paste("SA_im ~ x + Sf_imp_dur +", ev_name))
    model <- lm(formula, data = y)
    coefs <- summary(model)$coefficients

    if (ev_name %in% rownames(coefs)) {
      p_value <- coefs[ev_name, "Pr(>|t|)"]
      B_value <- coefs[ev_name, "Estimate"]

      if (!is.na(p_value) && p_value < 0.05) {
        y$Sf_imp_dur <- y$Sf_imp_dur + B_value * y[[ev_name]]
      }
    }
  }
 
 mode_results[[mode]] <- y
}
```

Visualizing creating a data frame with all spatial filters and
accessibilities:

```{r sf-df}
spatial_filter_imp_dur <- as.data.frame(bind_rows(mode_results, .id = "PwMode")) %>%
  dplyr::select(PRCDDA, PwMode, SA_im, Sf_imp_dur)
```

Pearson correlation between job spatial availability and spatial filter
by transportation mode:

```{r sf-correlations}
correlations <- spatial_filter_imp_dur %>%
  group_by(PwMode) %>%
  summarise(correlation = cor(SA_im, Sf_imp_dur, use = "complete.obs")) 

correlations
```

Creating a scatter plot figure:

```{r sf-plot}
r2_labels <- spatial_filter_imp_dur %>%
  group_by(PwMode) %>%
  summarise(r2 = summary(lm(Sf_imp_dur ~ SA_im))$r.squared) %>%
  mutate(label = paste0("R² = ", round(r2, 3)))

spatial_filter_imp_dur_fig <- ggplot(spatial_filter_imp_dur, aes(x = SA_im, y = Sf_imp_dur)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "lm", se = FALSE, color = "#db4f2c", linewidth = 0.55) +
  geom_text(data = r2_labels, 
            aes(x = -Inf, y = Inf, label = label), 
            hjust = -0.1, vjust = 1.1, inherit.aes = FALSE) +
  facet_wrap(~ PwMode, scales = "free") +
  theme_minimal() +
  labs(x = "Job spatial availability", y = "Spatial filter based on commute durantion impedance") + 
theme_minimal()

# Saving figure
ggsave(file = paste0(diretorio_export_figures,"/spatial_filter_imp_dur_scatterplot.jpg"), 
       plot = spatial_filter_imp_dur_fig,
       width = 16, 
       height = 9, 
       units = "cm", 
       dpi = 300)
```

```{r sf-figure}
knitr::include_graphics(paste0(diretorio_export_figures,"/spatial_filter_imp_dur_scatterplot.jpg"))
```

# Data export

Creating a unique file with all spatial filters (distance, median
commute duration, and impedance):

```{r}
spatial_filter <- spatial_filter_dist %>%
  left_join(spatial_filter_dur_medians[,c(1,2,4)], by = c("PRCDDA" = "PRCDDA", "PwMode" = "PwMode")) %>% 
  left_join(spatial_filter_imp_dur[,c(1,2,4)], by = c("PRCDDA" = "PRCDDA", "PwMode" = "PwMode")) 
```

Exporting the spatial filters (original file):

```{r export-spatial-filter-original}
write.csv(spatial_filter, paste0(diretorio_export_tables, "spatial_filter_original.csv"), row.names=FALSE)
```

If you are interested in manipulating the spatial filters outside a RDC
Office, the following codes will create a release version that it is in
accordance to the confidentiality vetting rules:

## Confidentiality vetting

Confidentiality vetting is the process of reviewing the results to be
released by the Research Data Center to ensure that confidentiality
risks for StatCan respondents are minimized.

The following rules apply to our results:

-   *Statistics must not be released for identifiable areas with less
    than 40 persons (*$\sum{CompW1} ≥ 40$). Statistics must not be
    released for identifiable areas with less than 40 persons. This
    condition is usually met with geography levels used at the RDCs. The
    population threshold can be applied to the (long form) weighted
    population estimate.

The block above checks the total population of each DA:

```{r da-vetting}
vetting_da_mode <- census %>%
  group_by(PRCDDA, PwMode) %>%
  dplyr::summarize(Weighted_pop_mode = sum(CompW1)) %>% 
         dplyr::select(PRCDDA, PwMode, Weighted_pop_mode)
```

Creating new files with the DA population:

```{r creating-release}
spatial_filter_support <- spatial_filter %>% 
  left_join(vetting_da_mode, by = c('PRCDDA' = 'PRCDDA','PwMode' = 'PwMode'))
```

Apply the rules to avoid confidentiality risks for the accessibility
tables:

```{r filter-support}
spatial_filter_support <- spatial_filter_support %>%
  filter(Weighted_pop_mode >= 40) 

spatial_filter_release <- spatial_filter_support %>%
  filter(Weighted_pop_mode >= 40) %>% 
  dplyr::select(-Weighted_pop_mode)
```

To finalize the methodology of this R markdown, we will export the
processed data.

```{r export-release-data}
write.csv(spatial_filter_support, paste0(diretorio_export_tables, "spatial_filter_support.csv"), row.names=FALSE)

write.csv(spatial_filter_release, paste0(diretorio_export_tables, "spatial_filter_release.csv"), row.names=FALSE)
```
